"""
Module de stockage CSV pour charger et exporter les fichiers CSV Shopify depuis/vers la base de donn√©es.
"""

import pandas as pd
import json
import logging
from typing import List, Dict, Optional, Set
from pathlib import Path

logger = logging.getLogger(__name__)

# Colonne obligatoire Shopify
REQUIRED_SHOPIFY_COLUMN = 'Handle'

# Colonnes Shopify typiques (au moins quelques-unes doivent √™tre pr√©sentes)
TYPICAL_SHOPIFY_COLUMNS = [
    'Title',
    'Body (HTML)',
    'Vendor',
    'Type',
    'Tags',
    'Variant SKU',
    'Variant Price',
    'Variant Compare At Price',
    'Variant Inventory Qty',
    'Image Src',
    'Image Position',
    'SEO Title',
    'SEO Description',
    'Google Shopping / Google Product Category'
]


class CSVStorage:
    """Gestionnaire de stockage CSV dans la base de donn√©es."""
    
    def __init__(self, db):
        """
        Initialise le gestionnaire de stockage CSV.
        
        Args:
            db: Instance de AIPromptsDB
        """
        self.db = db
    
    def import_csv(self, csv_path: str) -> int:
        """
        Charge un CSV Shopify dans la base de donn√©es.
        ATTENTION: Supprime TOUS les imports existants avant d'importer le nouveau fichier.
        
        Args:
            csv_path: Chemin vers le fichier CSV √† importer
            
        Returns:
            csv_import_id: ID de l'import cr√©√©
            
        Raises:
            ValueError: Si le fichier n'est pas au format Shopify
        """
        logger.info(f"Import du CSV: {csv_path}")
        
        # Lire le CSV
        try:
            df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)
        except Exception as e:
            logger.error(f"Erreur lors de la lecture du CSV: {e}")
            raise
        
        # VALIDATION 1: V√©rifier que Handle existe (obligatoire)
        if REQUIRED_SHOPIFY_COLUMN not in df.columns:
            error_msg = (
                "Le fichier n'est pas au format Shopify.\n"
                f"La colonne '{REQUIRED_SHOPIFY_COLUMN}' est obligatoire."
            )
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        # VALIDATION 2: V√©rifier qu'au moins quelques colonnes Shopify typiques existent
        present_columns = [col for col in TYPICAL_SHOPIFY_COLUMNS if col in df.columns]
        if len(present_columns) < 2:
            error_msg = (
                "Le fichier ne semble pas √™tre au format Shopify.\n"
                "Aucune colonne Shopify typique d√©tect√©e.\n"
                f"Colonnes pr√©sentes: {', '.join(df.columns[:10])}"
            )
            logger.error(error_msg)
            raise ValueError(error_msg)
        
        total_rows = len(df)
        logger.info(f"CSV valid√©: {total_rows} lignes, {len(df.columns)} colonnes")
        logger.info(f"Colonnes Shopify d√©tect√©es: {', '.join(present_columns)}")
        
        # üóëÔ∏è SUPPRIMER TOUS LES IMPORTS EXISTANTS avant d'importer le nouveau
        self.db.clear_all_imports()
        
        # Cr√©er le nouvel import dans la base de donn√©es
        csv_import_id = self.db.create_csv_import(csv_path, total_rows)
        
        # Ins√©rer chaque ligne dans csv_rows
        cursor = self.db.conn.cursor()
        
        for idx, row in df.iterrows():
            # Convertir la ligne en dictionnaire
            row_dict = row.to_dict()
            
            # Extraire le handle pour indexation rapide
            handle = row_dict.get('Handle', '')
            
            # Stocker toutes les colonnes en JSON
            data_json = json.dumps(row_dict, ensure_ascii=False)
            
            cursor.execute('''
                INSERT OR REPLACE INTO csv_rows 
                (csv_import_id, row_index, handle, data_json)
                VALUES (?, ?, ?, ?)
            ''', (csv_import_id, int(idx), handle, data_json))
        
        self.db.conn.commit()
        logger.info(f"Import termin√©: {total_rows} lignes stock√©es dans la base de donn√©es")
        
        return csv_import_id
    
    def get_csv_rows(self, csv_import_id: int, handles: Optional[Set[str]] = None) -> List[Dict]:
        """
        R√©cup√®re les lignes CSV depuis la base de donn√©es.
        
        Args:
            csv_import_id: ID de l'import CSV
            handles: Set de handles √† filtrer (optionnel, None = toutes les lignes)
            
        Returns:
            Liste de dictionnaires contenant les donn√©es des lignes
        """
        cursor = self.db.conn.cursor()
        
        if handles:
            # Filtrer par handles
            placeholders = ','.join(['?'] * len(handles))
            cursor.execute(f'''
                SELECT * FROM csv_rows 
                WHERE csv_import_id = ? AND handle IN ({placeholders})
                ORDER BY row_index
            ''', (csv_import_id, *handles))
        else:
            # Toutes les lignes
            cursor.execute('''
                SELECT * FROM csv_rows 
                WHERE csv_import_id = ?
                ORDER BY row_index
            ''', (csv_import_id,))
        
        rows = []
        for row in cursor.fetchall():
            row_dict = dict(row)
            # D√©coder le JSON
            row_dict['data'] = json.loads(row_dict['data_json'])
            rows.append(row_dict)
        
        return rows
    
    def get_unique_handles(self, csv_import_id: int) -> List[str]:
        """
        R√©cup√®re la liste des handles uniques pour un import CSV.
        
        Args:
            csv_import_id: ID de l'import CSV
            
        Returns:
            Liste des handles uniques (tri√©s)
        """
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT DISTINCT handle FROM csv_rows 
            WHERE csv_import_id = ? AND handle IS NOT NULL AND handle != ''
            ORDER BY handle
        ''', (csv_import_id,))
        
        return [row['handle'] for row in cursor.fetchall()]
    
    def update_csv_row(self, csv_row_id: int, field_updates: Dict[str, str]):
        """
        Met √† jour des champs sp√©cifiques d'une ligne CSV.
        
        Args:
            csv_row_id: ID de la ligne √† mettre √† jour
            field_updates: Dictionnaire {field_name: new_value}
        """
        cursor = self.db.conn.cursor()
        
        # R√©cup√©rer la ligne actuelle
        cursor.execute('SELECT data_json FROM csv_rows WHERE id = ?', (csv_row_id,))
        row = cursor.fetchone()
        if not row:
            raise ValueError(f"Ligne CSV {csv_row_id} introuvable")
        
        # D√©coder le JSON
        data = json.loads(row['data_json'])
        
        # Mettre √† jour les champs
        for field_name, new_value in field_updates.items():
            data[field_name] = new_value
        
        # Encoder et sauvegarder
        data_json = json.dumps(data, ensure_ascii=False)
        cursor.execute('''
            UPDATE csv_rows 
            SET data_json = ?, updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        ''', (data_json, csv_row_id))
        
        self.db.conn.commit()
        logger.debug(f"Ligne CSV {csv_row_id} mise √† jour: {list(field_updates.keys())}")
    
    def update_csv_row_status(self, csv_row_id: int, status: str, error_message: str = None, ai_explanation: str = None):
        """
        Met √† jour le status, error_message et ai_explanation d'une ligne CSV.
        
        Args:
            csv_row_id: ID de la ligne √† mettre √† jour
            status: Nouveau status ('pending', 'processing', 'completed', 'error')
            error_message: Message d'erreur court (optionnel)
            ai_explanation: Explication compl√®te de l'IA (optionnel)
        """
        cursor = self.db.conn.cursor()
        
        cursor.execute('''
            UPDATE csv_rows 
            SET status = ?, 
                error_message = ?, 
                ai_explanation = ?,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        ''', (status, error_message, ai_explanation, csv_row_id))
        
        self.db.conn.commit()
        logger.debug(f"Ligne CSV {csv_row_id} - Status: {status}")
    
    def get_row_by_id(self, csv_row_id: int) -> Optional[Dict]:
        """
        R√©cup√®re une ligne CSV par son ID.
        
        Args:
            csv_row_id: ID de la ligne
            
        Returns:
            Dictionnaire avec les donn√©es de la ligne (None si introuvable)
        """
        cursor = self.db.conn.cursor()
        cursor.execute('SELECT * FROM csv_rows WHERE id = ?', (csv_row_id,))
        row = cursor.fetchone()
        
        if not row:
            return None
        
        row_dict = dict(row)
        row_dict['data'] = json.loads(row_dict['data_json'])
        return row_dict
    
    def get_row_id_by_handle(self, csv_import_id: int, handle: str) -> Optional[int]:
        """
        R√©cup√®re l'ID d'une ligne CSV par son handle.
        
        Args:
            csv_import_id: ID de l'import CSV
            handle: Handle du produit
            
        Returns:
            ID de la ligne (None si introuvable)
        """
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT id FROM csv_rows 
            WHERE csv_import_id = ? AND handle = ?
            LIMIT 1
        ''', (csv_import_id, handle))
        row = cursor.fetchone()
        
        return row['id'] if row else None
    
    def get_rows_by_status(self, csv_import_id: int, status: str) -> List[Dict]:
        """
        R√©cup√®re les lignes CSV par status.
        
        Args:
            csv_import_id: ID de l'import CSV
            status: Status √† filtrer ('pending', 'processing', 'completed', 'error')
            
        Returns:
            Liste de dictionnaires contenant les donn√©es des lignes
        """
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT * FROM csv_rows 
            WHERE csv_import_id = ? AND status = ?
            ORDER BY row_index
        ''', (csv_import_id, status))
        
        rows = []
        for row in cursor.fetchall():
            row_dict = dict(row)
            row_dict['data'] = json.loads(row_dict['data_json'])
            rows.append(row_dict)
        
        return rows
    
    def get_status_summary(self, csv_import_id: int) -> Dict[str, int]:
        """
        R√©cup√®re le r√©sum√© des status pour un import CSV (compte les produits uniques, pas les lignes).
        
        Args:
            csv_import_id: ID de l'import CSV
            
        Returns:
            Dictionnaire {status: count} o√π count = nombre de produits uniques (handles)
        """
        cursor = self.db.conn.cursor()
        cursor.execute('''
            SELECT status, COUNT(DISTINCT handle) as count 
            FROM csv_rows 
            WHERE csv_import_id = ?
            GROUP BY status
        ''', (csv_import_id,))
        
        summary = {}
        for row in cursor.fetchall():
            status = row['status'] if row['status'] else 'pending'
            summary[status] = row['count']
        
        return summary
    
    def export_csv(self, csv_import_id: int, output_path: str):
        """
        Exporte le CSV depuis la base de donn√©es vers un fichier.
        
        Args:
            csv_import_id: ID de l'import CSV
            output_path: Chemin du fichier CSV de sortie
        """
        logger.info(f"Export du CSV vers: {output_path}")
        
        # R√©cup√©rer toutes les lignes
        rows = self.get_csv_rows(csv_import_id)
        
        if not rows:
            raise ValueError(f"Aucune ligne trouv√©e pour l'import {csv_import_id}")
        
        # Reconstruire le DataFrame
        data_list = [row['data'] for row in rows]
        df = pd.DataFrame(data_list)
        
        # R√©ordonner les colonnes selon SHOPIFY_ALL_COLUMNS si disponible
        try:
            from csv_config import SHOPIFY_ALL_COLUMNS
            # Garder l'ordre des colonnes d√©finies dans SHOPIFY_ALL_COLUMNS
            # Ajouter les colonnes manquantes √† la fin
            ordered_columns = [col for col in SHOPIFY_ALL_COLUMNS if col in df.columns]
            other_columns = [col for col in df.columns if col not in ordered_columns]
            df = df[ordered_columns + other_columns]
        except ImportError:
            logger.warning("csv_config non disponible, utilisation de l'ordre par d√©faut")
        
        # Sauvegarder le CSV
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"CSV export√©: {len(df)} lignes, {len(df.columns)} colonnes")
        
        return output_path
    
    def get_last_import(self) -> Optional[Dict]:
        """
        R√©cup√®re le dernier import CSV de la base de donn√©es.
        
        Returns:
            Dict avec les cl√©s: id, original_file_path, imported_at, total_rows
            None si aucun import trouv√©
        """
        cursor = self.db.conn.cursor()
        cursor.execute("""
            SELECT id, original_file_path, imported_at, total_rows
            FROM csv_imports
            ORDER BY imported_at DESC
            LIMIT 1
        """)
        
        result = cursor.fetchone()
        
        if not result:
            return None
        
        return {
            'id': result[0],
            'original_file_path': result[1],
            'imported_at': result[2],
            'total_rows': result[3]
        }
